{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import plotly.express as px\n",
    "\n",
    "# import plotly.express as px\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely.ops import nearest_points  # Correct import path\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Local libraries\n",
    "\n",
    "sys.path.insert(0, '/Volumes/RyanMercerTB3/dev_RyanMercer')\n",
    "\n",
    "from locallib.dataframe_plotly import (plot_stacked_time_series_df,\n",
    "                                       plot_overlayed_time_series_df)\n",
    "from locallib.dataframe_utils import (print_df_size_in_mb, \n",
    "                                     resample_data_frame)\n",
    "from locallib.time_series_utils import (pad_short_time_series,\n",
    "                                       nearest_neighbor_selection,\n",
    "                                       plot_query_nn,\n",
    "                                       plot_time_series,\n",
    "                                       plot_stacked_time_series_list,)\n",
    "from locallib.plotting.plot_overlayed_time_series_df import (plot_overlayed_time_series_df)\n",
    "from locallib import (novelets, \n",
    "                      contrast_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_info = [\n",
    "    {'enable': False, \n",
    "     'site_name': 'Calumet SWD', \n",
    "     'site_id': 33614, 'num_pumps': 1, \n",
    "     'site_dir_name': 'CalumetSWD(33614)', \n",
    "     'pump_curve_path': '/Users/audreyder/Bison_Water/AllPumpCSV/PumpCurve_Calument_33614_DataPoints.csv', # Ryan & DLT, replace these paths as appropriate - Audrey\n",
    "     'calibration_stages': [\n",
    "         {'frequency': 49, \n",
    "          'start_time': '2024-12-13 12:00:19', \n",
    "          'end_time': '2024-12-13 13:55:19'}, \n",
    "          \n",
    "          {'frequency': 51, \n",
    "           'start_time': '2024-12-13 14:00:19', \n",
    "           'end_time': '2024-12-13 15:40:19'}, \n",
    "           \n",
    "           {'frequency': 53, \n",
    "            'start_time': '2024-12-13 15:45:21', \n",
    "            'end_time': '2024-12-13 17:40:20'}, \n",
    "            \n",
    "            {'frequency': 55, \n",
    "             'start_time': '2024-12-13 17:45:21', \n",
    "             'end_time': '2024-12-13 19:35:20'}]}, \n",
    "             \n",
    "     {'enable': False, \n",
    "      'site_name': 'Canadian SWD', \n",
    "      'site_id': 57740, \n",
    "      'num_pumps': 1, \n",
    "      'site_dir_name': 'CanadianSWD(57740)', \n",
    "      'pump_curve_path': '/Users/audreyder/Bison_Water/AllPumpCSV/PumpCurve_Canadian_57740_DataPoints.csv', # Ryan & DLT, replace these paths as appropriate - Audrey\n",
    "      'calibration_stages': [\n",
    "          {'frequency': 44, \n",
    "           'start_time': '2024-12-13 12:00:02', \n",
    "           'end_time': '2024-12-13 13:58:02'}, \n",
    "           \n",
    "           {'frequency': 47, \n",
    "            'start_time': '2024-12-13 13:59:03', \n",
    "            'end_time': '2024-12-13 15:51:03'}, \n",
    "            \n",
    "           {'frequency': 49, \n",
    "             'start_time': '2024-12-13 15:52:03', \n",
    "             'end_time': '2024-12-13 17:51:03'}, \n",
    "             \n",
    "           {'frequency': 51, \n",
    "            'start_time': '2024-12-13 17:52:02', \n",
    "            'end_time': '2024-12-13 19:50:04'}, \n",
    "            \n",
    "           {'frequency': 53, \n",
    "            'start_time': '2024-12-13 19:51:03', \n",
    "            'end_time': '2024-12-13 21:47:04'}, \n",
    "            \n",
    "           {'frequency': 55, \n",
    "            'start_time': '2024-12-13 21:48:03', \n",
    "            'end_time': '2024-12-13 23:53:02'}, \n",
    "            \n",
    "           {'frequency': 57, \n",
    "            'start_time': '2024-12-13 23:54:03', \n",
    "            'end_time': '2024-12-14 01:53:03'}, \n",
    "            \n",
    "           {'frequency': 59, \n",
    "            'start_time': '2024-12-14 01:54:03', \n",
    "            'end_time': '2024-12-14 03:53:02'}]\n",
    "     }, \n",
    "            \n",
    "     {'enable': False, \n",
    "      'site_name': 'Siegrist SWD', \n",
    "      'site_id': 33467, \n",
    "      'num_pumps': 1, \n",
    "      'site_dir_name': 'SiegristSWD(33467)', \n",
    "      'pump_curve_path': 'AllPumpCSV/PumpCurve_Siegrist_33467_DataPoints.csv', # Ryan & DLT, replace these paths as appropriate - Audrey\n",
    "      'calibration_stages': [\n",
    "            {'frequency': 47, \n",
    "             'start_time': '2024-12-17 10:00:19', \n",
    "             'end_time': '2024-12-17 12:00:21'}, \n",
    "             \n",
    "            {'frequency': 49, \n",
    "             'start_time': '2024-12-17 12:05:25', \n",
    "             'end_time': '2024-12-17 14:00:32'}, \n",
    "             \n",
    "            {'frequency': 51, \n",
    "             'start_time': '2024-12-17 14:01:18', \n",
    "             'end_time': '2024-12-17 16:00:19'}, \n",
    "             \n",
    "            {'frequency': 53, \n",
    "             'start_time': '2024-12-17 16:01:04', \n",
    "             'end_time': '2024-12-17 18:00:38'}, \n",
    "             \n",
    "            {'frequency': 55, \n",
    "             'start_time': '2024-12-17 18:00:58', \n",
    "             'end_time': '2024-12-17 20:00:22'}, \n",
    "             \n",
    "            {'frequency': 57, \n",
    "             'start_time': '2024-12-17 20:05:27', \n",
    "             'end_time': '2024-12-17 21:55:19'}]\n",
    "             \n",
    "     }, \n",
    "     \n",
    "     {'enable': False, \n",
    "      'site_name': 'Union City 2 SWD', \n",
    "      'site_id': 33404, \n",
    "      'num_pumps': 1, \n",
    "      'site_dir_name': 'UnionCity2SWD(33404)', \n",
    "      'pump_curve_path': '/Users/audreyder/Bison_Water/AllPumpCSV/PumpCurve_UnionCity2_33404_DataPoints.csv', # Ryan & DLT, replace these paths as appropriate - Audrey\n",
    "      'calibration_stages': [\n",
    "          {'frequency': 46, \n",
    "           'start_time': '2024-12-17 12:00:12', \n",
    "           'end_time': '2024-12-17 13:50:12'}, \n",
    "           \n",
    "          {'frequency': 48, \n",
    "           'start_time': '2024-12-17 13:55:11', \n",
    "           'end_time': '2024-12-17 15:50:11'}, \n",
    "           \n",
    "          {'frequency': 50, \n",
    "           'start_time': '2024-12-17 15:55:11', \n",
    "           'end_time': '2024-12-17 17:50:12'}, \n",
    "           \n",
    "          {'frequency': 52, \n",
    "           'start_time': '2024-12-17 17:55:12', \n",
    "           'end_time': '2024-12-17 19:50:11'}, \n",
    "           \n",
    "          {'frequency': 54, \n",
    "           'start_time': '2024-12-17 19:55:11', \n",
    "           'end_time': '2024-12-17 21:45:11'}, \n",
    "           \n",
    "          {'frequency': 56, \n",
    "           'start_time': '2024-12-17 21:50:11', \n",
    "           'end_time': '2024-12-17 23:35:14'}]\n",
    "           \n",
    "     }\n",
    "     \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     '57740': 'Canadian SWD',\n",
    "#     '33614': 'Calumet SWD',\n",
    "#     '33404': 'Union City 2 SWD',\n",
    "#     '33467': 'Siegrist SWD',\n",
    "#     # '50248': 'Seiling SWD',\n",
    "#     # '57651': 'American Horse SWD',\n",
    "#     # '60818': 'Brown SWD',\n",
    "#     # '60385': 'BIG 4 SWD',\n",
    "#     # '53764': 'Okarche South SWD',\n",
    "#     # '54563': 'Okarche North SWD',\n",
    "#     # '50208': 'Alpha SWD',\n",
    "#     # '50252': 'Red Winter SWD ',\n",
    "#     # '54727': 'Kingfisher SWD Pump 1',\n",
    "#     # '54728': 'Kingfisher SWD Pump 2',\n",
    "#     # '48054': '3 mile SWD',\n",
    "\n",
    "# sites_info = [\n",
    "#     {'site_name':'Canadian SWD',   'site_id': 57740, 'num_pumps': 1, 'site_dir_name': 'CanadianSWD(57740)'},\n",
    "#     # {'site_name':'Union City 2 SWD', 'site_id': 33404, 'num_pumps': 1, 'site_dir_name': 'UnionCity2SWD(33404)'},\n",
    "#     # {'site_name':'Siegrist SWD',   'site_id': 33467, 'num_pumps': 1, 'site_dir_name': 'SiegristSWD(33467)'},\n",
    "#     # {'site_name':'Calumet SWD',   'site_id': 33614, 'num_pumps': 1, 'site_dir_name': 'CalumetSWD(33614)'},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_csv_file.csv' with the path to your CSV file\n",
    "enable_site_idx = 3\n",
    "for site_idx in range(len(sites_info)):\n",
    "    sites_info[site_idx]['enable'] = site_idx == enable_site_idx\n",
    "    \n",
    "site_info = sites_info[enable_site_idx]\n",
    "csv_file_path = site_info['pump_curve_path']\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df_pump_data = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_facility_map = {\n",
    "    \n",
    "#     '57740': 'Canadian SWD',\n",
    "#     '33614': 'Calumet SWD',\n",
    "#     '33404': 'Union City 2 SWD',\n",
    "#     '33467': 'Siegrist SWD',\n",
    "#     # '50248': 'Seiling SWD',\n",
    "#     # '57651': 'American Horse SWD',\n",
    "#     # '60818': 'Brown SWD',\n",
    "#     # '60385': 'BIG 4 SWD',\n",
    "#     # '53764': 'Okarche South SWD',\n",
    "#     # '54563': 'Okarche North SWD',\n",
    "#     # '50208': 'Alpha SWD',\n",
    "#     # '50252': 'Red Winter SWD ',\n",
    "#     # '54727': 'Kingfisher SWD Pump 1',\n",
    "#     # '54728': 'Kingfisher SWD Pump 2',\n",
    "#     # '48054': '3 mile SWD',\n",
    "    \n",
    "    \n",
    "#     # Add more device IDs and facility names here\n",
    "# }\n",
    "\n",
    "# 1. Device Selection Options\n",
    "# [\n",
    "#     57740, #'Canadian SWD'\n",
    "#     # 48034,\n",
    "#     # 33614, #'Calumet SWD',\n",
    "#     # 33404, #'Union City 2 SWD'\n",
    "#     # 33467, #'Siegrist SWD'    \n",
    "# ]  # Li\n",
    "device_ids = [site_info['site_id'] for site_info in sites_info if site_info['enable']==True]\n",
    "\n",
    "# st of device IDs (integers), e.g., [57740, 33614]\n",
    "device_name_substrings = []#['canadian']  # List of devicename substrings, e.g., ['canadian', 'alpha']\n",
    "\n",
    "# 2. Time Zone\n",
    "time_zone = 'UTC'\n",
    "\n",
    "# 3. Measurement Mappings\n",
    "measurements = {\n",
    "    # 'Vibration': ['sv/pmp_vib_rear', 'sv/hp1_vib', 'sv/vib'],\n",
    "    # 'Thrust Temperature': ['sv/thrust_temp'],\n",
    "    # 'Suction Pressure': ['sv/suctp'],\n",
    "    'Discharge Pressure': ['sv/discp'],\n",
    "    'Flow Rate': ['sv/fr'],\n",
    "    'Frequency': ['sv/vfd_speed', 'sv/HZ', 'sv/hp1_hz'],\n",
    "    # 'Amps': ['sv/vfd_current'],\n",
    "    # 'AmpA': ['sv/hp1_amps_a'],\n",
    "    # 'AmpsB': ['sv/hp1_amps_b'],\n",
    "    # 'AmpsC': ['sv/hp1_amps_c'],\n",
    "    # 'Volts': ['sv/vfd_voltage', 'sv/hp1_volts_a', 'sv/hp1_volts_b', 'sv/hp1_volts_c'],\n",
    "    # 'VoltsA': ['sv/hp1_volts_a'],\n",
    "    # 'VoltsB': ['sv/hp1_volts_b'],\n",
    "    # 'VoltsC': ['sv/hp1_volts_c'],\n",
    "    # 'Meter Total': ['sv/accum_volume', 'sv/av'],\n",
    "    # Add more measurements as needed\n",
    "}\n",
    "\n",
    "# 4. Time Range\n",
    "# time_interval = '1 hour'  # Adjust as needed\n",
    "\n",
    "# Alternatively, specify exact start and end times\n",
    "time_interval = None\n",
    "# start_time = datetime.now() - timedelta(days=10)\n",
    "# end_time = datetime.now()\n",
    "\n",
    "start_time = '2024-12-10'\n",
    "end_time = '2025-01-10'\n",
    "\n",
    "# Function to generate SQL query\n",
    "def generate_sql_query(device_ids, device_name_substrings, time_zone, measurements, time_interval):\n",
    "    # Build SELECT clauses for each measurement\n",
    "    select_clauses = [\n",
    "        \"h.deviceid AS site_id\",\n",
    "        \"d.devicename AS facility_name\",\n",
    "        \"CONVERT_TIMEZONE(%s, h.datetime) AS timestamp\"\n",
    "    ]\n",
    "    params = [time_zone]\n",
    "    where_paths = set()\n",
    "\n",
    "    for measurement_name, paths in measurements.items():\n",
    "        case_conditions = \"\\n            \".join(\n",
    "            [f\"WHEN h.path = '{path}' THEN h.value\" for path in paths]\n",
    "        )\n",
    "        select_clause = f\"\"\"MAX(\n",
    "                CASE\n",
    "                    {case_conditions}\n",
    "                    ELSE NULL\n",
    "                END\n",
    "            ) AS \"{measurement_name}\" \"\"\"\n",
    "        select_clauses.append(select_clause)\n",
    "        where_paths.update(paths)\n",
    "\n",
    "    # Build WHERE clause components\n",
    "    where_clauses = []\n",
    "    where_params = []\n",
    "\n",
    "    # Device ID filtering\n",
    "    if device_ids:\n",
    "        device_id_placeholders = ', '.join(['%s'] * len(device_ids))\n",
    "        where_clauses.append(f\"h.deviceid IN ({device_id_placeholders})\")\n",
    "        where_params.extend(device_ids)\n",
    "\n",
    "    # Device name substring filtering\n",
    "    if device_name_substrings:\n",
    "        name_conditions = []\n",
    "        for substring in device_name_substrings:\n",
    "            name_conditions.append(\"d.devicename ILIKE %s\")\n",
    "            where_params.append(f\"%{substring}%\")\n",
    "        where_clauses.append(\"(\" + \" OR \".join(name_conditions) + \")\")\n",
    "\n",
    "    # Only include devices that are not disabled\n",
    "    where_clauses.append(\"d.disabled = %s\")\n",
    "    where_params.append(False)\n",
    "\n",
    "    # h.path filtering\n",
    "    path_placeholders = ', '.join(['%s'] * len(where_paths))\n",
    "    where_clauses.append(f\"h.path IN ({path_placeholders})\")\n",
    "    where_params.extend(where_paths)\n",
    "\n",
    "    # Time filtering\n",
    "    # where_clauses.append(f\"h.datetime > GETDATE() - INTERVAL %s\")\n",
    "    # where_params.append(time_interval)\n",
    "    # Alternatively, for specific times:\n",
    "    where_clauses.append(f\"h.datetime BETWEEN %s AND %s\")\n",
    "    where_params.append(start_time)\n",
    "    where_params.append(end_time)\n",
    "\n",
    "    # Combine WHERE clause\n",
    "    where_clause = \"WHERE\\n    \" + \"\\n    AND \".join(where_clauses)\n",
    "\n",
    "    # Assemble the final query\n",
    "    query = f\"\"\"\n",
    "SELECT\n",
    "    {',    '.join(select_clauses)}\n",
    "FROM\n",
    "    historical AS h\n",
    "    JOIN devices AS d ON h.deviceid = d.deviceid\n",
    "{where_clause}\n",
    "GROUP BY\n",
    "    h.deviceid, d.devicename, h.datetime\n",
    "ORDER BY\n",
    "    h.deviceid, h.datetime;\n",
    "\"\"\"\n",
    "    # Combine all parameters\n",
    "    params.extend(where_params)\n",
    "    return query, params\n",
    "\n",
    "# Generate the query and parameters\n",
    "query, params = generate_sql_query(device_ids, device_name_substrings, time_zone, measurements, time_interval)\n",
    "# print(\"Generated SQL Query:\")\n",
    "# print(query)\n",
    "# print(\"Query Parameters:\")\n",
    "# print(params)\n",
    "\n",
    "# Set up your database connection (replace with your actual credentials)\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv('REDSHIFT_ENDPOINT'),\n",
    "    port=os.getenv('REDSHIFT_PORT'),\n",
    "    database=os.getenv('REDSHIFT_DBNAME'),\n",
    "    user=os.environ.get('REDSHIFT_USER'),\n",
    "    password=os.getenv('REDSHIFT_PASS')\n",
    ")\n",
    "\n",
    "# Execute the query and load results into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn, params=params)\n",
    "df['pump_id'] = 1\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "df.set_index(['site_id','pump_id','timestamp'],inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to ensure 'timestamp' is a datetime, reassign the index:\n",
    "df.index = pd.MultiIndex.from_arrays(\n",
    "    [\n",
    "        df.index.get_level_values('site_id'),\n",
    "        df.index.get_level_values('pump_id'),\n",
    "        pd.to_datetime(df.index.get_level_values('timestamp'))\n",
    "    ],\n",
    "    names=['site_id', 'pump_id', 'timestamp']\n",
    ")\n",
    "\n",
    "# If you need a date filter, you can do something like:\n",
    "# start_date = '2024-10-10'\n",
    "# end_date = df.index.get_level_values('timestamp').max()\n",
    "# df = df.loc[(slice(None), slice(None), slice(start_date, end_date)), :]\n",
    "\n",
    "# Define pump-related variables\n",
    "pressure_column = 'discharge pressure'\n",
    "flow_column = 'flow rate'\n",
    "frequency_float_column = 'frequency'\n",
    "frequency_int_column = 'frequency int'\n",
    "\n",
    "freq_min = 40\n",
    "freq_max = 60\n",
    "\n",
    "# Apply filtering masks on the main DataFrame (df)\n",
    "mask = (df[frequency_float_column].astype(float) >= freq_min) & (df[frequency_float_column].astype(float) <= freq_max)\n",
    "mask &= df[flow_column].astype(float) < 30000\n",
    "mask &= df[flow_column].astype(float) > 5000\n",
    "# If needed:\n",
    "# mask &= df[pressure_column].astype(float) > 1300\n",
    "\n",
    "df = df[mask]\n",
    "\n",
    "df[frequency_int_column] = df[frequency_float_column].round().astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary column names\n",
    "flow_column = 'flow rate'          # Replace with your actual flow rate column name\n",
    "pressure_column = 'discharge pressure'  # Replace with your actual pressure (TDH) column name\n",
    "frequency_column = 'frequency'     # Replace with your actual frequency column name\n",
    "frequency_int_column = 'frequency_int'  # If you have integer frequencies\n",
    "timestamp_column = 'timestamp'     # Replace with your actual timestamp column name\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Separate speed and efficiency lines\n",
    "speed_data = df_pump_data[df_pump_data['type'] == 'speed']\n",
    "efficiency_data = df_pump_data[df_pump_data['type'] == 'efficiency']\n",
    "\n",
    "# Get unique labels for speed and efficiency lines\n",
    "speed_lines = sorted(speed_data['label'].unique(), key=lambda x: float(x))\n",
    "efficiency_lines = [line for line in sorted(efficiency_data['label'].unique()) if 'triangle' not in line.lower()]\n",
    "\n",
    "# Create interpolation functions for each speed line\n",
    "speed_line_funcs = {}\n",
    "for label in speed_lines:\n",
    "    line_data = speed_data[speed_data['label'] == label].sort_values('x')\n",
    "    x_speed = line_data['x'].values\n",
    "    y_speed = line_data['y'].values\n",
    "    speed_line_funcs[label] = interp1d(x_speed, y_speed, bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "# Create interpolation functions for each efficiency line\n",
    "efficiency_line_funcs = {}\n",
    "for label in efficiency_lines:\n",
    "    line_data = efficiency_data[efficiency_data['label'] == label].sort_values('x')\n",
    "    x_efficiency = line_data['x'].values\n",
    "    y_efficiency = line_data['y'].values\n",
    "    efficiency_line_funcs[label] = interp1d(x_efficiency, y_efficiency, bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "# Find intersection points between speed lines and efficiency lines\n",
    "intersection_points = {}\n",
    "\n",
    "def find_intersection_point(func1, func2, x_min, x_max):\n",
    "    from scipy.optimize import brentq\n",
    "    def func_diff(x):\n",
    "        return func1(x) - func2(x)\n",
    "    try:\n",
    "        x_intersect = brentq(func_diff, x_min, x_max)\n",
    "        y_intersect = func1(x_intersect)\n",
    "        return x_intersect, y_intersect\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "for speed_line in speed_lines:\n",
    "    speed_func = speed_line_funcs[speed_line]\n",
    "    x_speed_min = speed_data[speed_data['label'] == speed_line]['x'].min()\n",
    "    x_speed_max = speed_data[speed_data['label'] == speed_line]['x'].max()\n",
    "    intersection_points[speed_line] = {}\n",
    "    for efficiency_line in efficiency_lines:\n",
    "        eff_func = efficiency_line_funcs[efficiency_line]\n",
    "        x_eff_min = efficiency_data[efficiency_data['label'] == efficiency_line]['x'].min()\n",
    "        x_eff_max = efficiency_data[efficiency_data['label'] == efficiency_line]['x'].max()\n",
    "        x_min = max(x_speed_min, x_eff_min)\n",
    "        x_max = min(x_speed_max, x_eff_max)\n",
    "        if x_min < x_max:\n",
    "            intersection = find_intersection_point(speed_func, eff_func, x_min, x_max)\n",
    "            if intersection is not None:\n",
    "                intersection_points[speed_line][efficiency_line] = intersection\n",
    "            else:\n",
    "                print(f\"No intersection found between speed line {speed_line} and efficiency line {efficiency_line}\")\n",
    "        else:\n",
    "            print(f\"No overlapping x range between speed line {speed_line} and efficiency line {efficiency_line}\")\n",
    "\n",
    "# Interpolate between Min to BEP and BEP to Max for each speed line\n",
    "interpolated_speed_line = {}\n",
    "\n",
    "for speed_line in speed_lines:\n",
    "    if all(key in intersection_points[speed_line] for key in ['Min', 'BEP', 'Max']):\n",
    "        min_point = intersection_points[speed_line]['Min']\n",
    "        bep_point = intersection_points[speed_line]['BEP']\n",
    "        max_point = intersection_points[speed_line]['Max']\n",
    "    else:\n",
    "        print(f\"Missing intersection points for speed line {speed_line}, skipping\")\n",
    "        continue\n",
    "\n",
    "    speed_func = speed_line_funcs[speed_line]\n",
    "\n",
    "    # Interpolate between Min and BEP\n",
    "    x_min_bep = np.linspace(min_point[0], bep_point[0], 100)\n",
    "    y_min_bep = speed_func(x_min_bep)\n",
    "    health_score_min_bep = np.linspace(-100, 0, 100)\n",
    "\n",
    "    # Interpolate between BEP and Max\n",
    "    x_bep_max = np.linspace(bep_point[0], max_point[0], 100)\n",
    "    y_bep_max = speed_func(x_bep_max)\n",
    "    health_score_bep_max = np.linspace(0, 100, 100)\n",
    "\n",
    "    # Combine the two\n",
    "    x_interp = np.concatenate([x_min_bep, x_bep_max])\n",
    "    y_interp = np.concatenate([y_min_bep, y_bep_max])\n",
    "    health_score = np.concatenate([health_score_min_bep, health_score_bep_max])\n",
    "\n",
    "    interpolated_speed_line[speed_line] = pd.DataFrame({\n",
    "        'flow_rate': x_interp,\n",
    "        'tdh': y_interp,\n",
    "        'health_score': health_score\n",
    "    })\n",
    "\n",
    "# Create interpolation functions for each speed line\n",
    "health_score_functions = {}\n",
    "for speed_line in interpolated_speed_line:\n",
    "    df_line = interpolated_speed_line[speed_line]\n",
    "    df_line = df_line.drop_duplicates(subset='flow_rate').sort_values('flow_rate')\n",
    "    flow_rate = df_line['flow_rate'].values\n",
    "    health_score = df_line['health_score'].values\n",
    "    health_score_func = interp1d(\n",
    "        flow_rate, health_score, bounds_error=False, fill_value=(health_score[0], health_score[-1])\n",
    "    )\n",
    "    health_score_functions[speed_line] = health_score_func\n",
    "\n",
    "# Extract numeric frequencies from speed lines\n",
    "speed_line_freqs = np.array([float(speed_line) for speed_line in speed_lines])\n",
    "\n",
    "# Function to compute health score for each time sample\n",
    "def compute_health_score(row):\n",
    "    f = row[frequency_column]\n",
    "    q = row[flow_column]\n",
    "\n",
    "    # Check if frequency is within the desired range\n",
    "    if f < freq_min or f > freq_max:\n",
    "        return np.nan  # Exclude frequencies outside the range\n",
    "\n",
    "    # Find two nearest speed lines\n",
    "    freq_array = speed_line_freqs\n",
    "    if f <= freq_array.min():\n",
    "        f1 = f2 = freq_array.min()\n",
    "    elif f >= freq_array.max():\n",
    "        f1 = f2 = freq_array.max()\n",
    "    else:\n",
    "        idx = np.searchsorted(freq_array, f)\n",
    "        f1 = freq_array[idx - 1]\n",
    "        f2 = freq_array[idx]\n",
    "\n",
    "    f1_str = str(int(f1)) if f1 == int(f1) else str(f1)\n",
    "    f2_str = str(int(f2)) if f2 == int(f2) else str(f2)\n",
    "\n",
    "    # Compute weights\n",
    "    if f1 == f2:\n",
    "        weight1 = 1.0\n",
    "        weight2 = 0.0\n",
    "    else:\n",
    "        weight1 = (f2 - f) / (f2 - f1)\n",
    "        weight2 = (f - f1) / (f2 - f1)\n",
    "\n",
    "    # Get health scores at flow rate q\n",
    "    try:\n",
    "        health_score_f1 = health_score_functions[f1_str](q)\n",
    "    except KeyError:\n",
    "        print(f\"Speed line {f1_str} not found\")\n",
    "        return np.nan\n",
    "    except ValueError:\n",
    "        return np.nan  # Flow rate q is outside interpolation range\n",
    "    try:\n",
    "        health_score_f2 = health_score_functions[f2_str](q)\n",
    "    except KeyError:\n",
    "        print(f\"Speed line {f2_str} not found\")\n",
    "        return np.nan\n",
    "    except ValueError:\n",
    "        return np.nan  # Flow rate q is outside interpolation range\n",
    "\n",
    "    # Compute final health score\n",
    "    health_score = weight1 * health_score_f1 + weight2 * health_score_f2\n",
    "    return health_score\n",
    "\n",
    "# Assume 'df' is already indexed by ['site_id','pump_id','timestamp'] and has columns:\n",
    "# flow_column, pressure_column, frequency_column, and timestamp_column\n",
    "\n",
    "# Convert frequency to integer if necessary\n",
    "df[frequency_int_column] = df[frequency_column].astype(int)\n",
    "\n",
    "# Filter the DataFrame to only include frequencies between the lowest and highest speed lines\n",
    "freq_min = speed_line_freqs.min()\n",
    "freq_max = speed_line_freqs.max()\n",
    "df = df[(df[frequency_column] >= freq_min) & (df[frequency_column] <= freq_max)]\n",
    "\n",
    "# Update the list of frequencies after filtering\n",
    "df[frequency_int_column] = df[frequency_column].astype(int)\n",
    "frequencies = sorted(df[frequency_int_column].unique())\n",
    "\n",
    "# Apply the function to compute health score for each row\n",
    "df['health_score'] = df.apply(compute_health_score, axis=1)\n",
    "\n",
    "# Remove rows with NaN health scores (which may result from frequencies outside the range)\n",
    "df = df.dropna(subset=['health_score'])\n",
    "\n",
    "# Create df_plot with timestamp as the index while minimally changing the code\n",
    "df_plot = df.reset_index().set_index([timestamp_column])\n",
    "df_plot[frequency_int_column] = df_plot[frequency_column].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colormapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color map for frequencies over 40 to 60 Hz\n",
    "# Frequencies outside 40-60 Hz are already excluded\n",
    "\n",
    "\n",
    "frequencies = np.arange(60,39,-1)# sorted([f for f in df_plot[frequency_int_column].unique() if freq_min <= f <= freq_max],reverse=True)\n",
    "freq_min = np.min(frequencies)\n",
    "freq_max = np.max(frequencies)\n",
    "\n",
    "\n",
    "# Define the colorscale\n",
    "colorscale = px.colors.sequential.Turbo\n",
    "n_colors = len(colorscale)\n",
    "\n",
    "# Function to map frequency to color\n",
    "def map_frequency_to_color(freq):\n",
    "    # Map to color index\n",
    "    color_index = int((freq - freq_min) / (freq_max - freq_min) * (n_colors - 1))\n",
    "    return colorscale[color_index]\n",
    "\n",
    "# Create color_map, mapping frequencies to colors\n",
    "color_map = {freq: map_frequency_to_color(freq) for freq in frequencies}\n",
    "# color_map = {freq: '#000000' for freq in frequencies}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pump Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### Pump Curve ###\n",
    "##################\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "plot_slice = 100\n",
    "\n",
    "# Iterate over each site_id and pump_id pair\n",
    "for (site_id, pump_id), group_df in df.groupby(level=['site_id','pump_id']):\n",
    "    fig1 = go.Figure()\n",
    "\n",
    "    # Plot speed lines\n",
    "    for label in sorted(speed_lines, reverse=True, key=lambda x: float(x)):\n",
    "        line_data = speed_data[speed_data['label'] == label]\n",
    "        x_values = line_data['x']\n",
    "        y_values = line_data['y']\n",
    "        freq = int(label)\n",
    "        if freq not in color_map:\n",
    "            continue\n",
    "        line_color = color_map[freq]\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_values,\n",
    "                y=y_values,\n",
    "                mode='lines',\n",
    "                name=f'{label}Hz',\n",
    "                legendgroup=label,\n",
    "                showlegend=True,\n",
    "                line=dict(dash='dash', color=line_color)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Plot efficiency lines\n",
    "    for label in ['Min', 'BEP', 'Max']:\n",
    "        if label in efficiency_lines:\n",
    "            line_data = efficiency_data[efficiency_data['label'] == label]\n",
    "            x_values = line_data['x']\n",
    "            y_values = line_data['y']\n",
    "            fig1.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x_values,\n",
    "                    y=y_values,\n",
    "                    mode='lines',\n",
    "                    name=label,\n",
    "                    legendgroup=label,\n",
    "                    showlegend=False,\n",
    "                    line=dict(dash='solid', color='gray')\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Plot synthetic data points (using the subset group_df)\n",
    "    current_frequencies = sorted(group_df[frequency_int_column].unique())\n",
    "    for freq in current_frequencies:\n",
    "        freq_data = group_df[group_df[frequency_int_column] == freq]\n",
    "        if freq not in color_map:\n",
    "            continue\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                x=freq_data[flow_column][::plot_slice],\n",
    "                y=freq_data[pressure_column][::plot_slice],\n",
    "                mode='markers',\n",
    "                name=f'{int(freq)}Hz Data',\n",
    "                legendgroup=f'{int(freq)}',\n",
    "                marker=dict(color=color_map[freq]),\n",
    "                showlegend=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout for the current site_id and pump_id\n",
    "    fig1.update_layout(\n",
    "        title=f'Pump Curve over Time (Site: {site_id}, Pump: {pump_id})',\n",
    "        xaxis_title='Flow Rate (BBL/day)',\n",
    "        yaxis_title='TDH (PSI)',\n",
    "        template='plotly_white',\n",
    "        height=800,\n",
    "        width=950,\n",
    "    )\n",
    "\n",
    "    # Display the figure for this site_id, pump_id pair\n",
    "    fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series of Percent Deviation from BEP\n",
    "\n",
    "Shown as a scatterplot with markers color coded to the frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### Health Score Time Series ###\n",
    "#################\n",
    "plot_slice = 10\n",
    "\n",
    "# start_time = '2024-12-13'\n",
    "# end_time = '2024-12-15'\n",
    "# Iterate over each site_id and pump_id pair\n",
    "for (site_id, pump_id), group_df in df.groupby(level=['site_id','pump_id']):\n",
    "    # For each group, reset index to timestamp\n",
    "    df_plot = group_df.reset_index().set_index(timestamp_column)#.loc[start_time:end_time]\n",
    "\n",
    "    fig3 = go.Figure()\n",
    "\n",
    "    # Get the unique frequencies for the current group\n",
    "    current_frequencies = sorted(df_plot[frequency_int_column].unique(), reverse=True)\n",
    "\n",
    "    for freq in current_frequencies:\n",
    "        freq_data = df_plot[df_plot[frequency_int_column] == freq]\n",
    "        # Only plot if freq is in color_map\n",
    "        if freq not in color_map:\n",
    "            continue\n",
    "        fig3.add_trace(\n",
    "            go.Scatter(\n",
    "                x=freq_data[::plot_slice].index,  # Use index as x-axis (timestamp)\n",
    "                y=freq_data['health_score'][::plot_slice],\n",
    "                mode='markers',\n",
    "                name=f'{int(freq)}Hz',\n",
    "                marker=dict(color=color_map[freq]),\n",
    "                legendgroup=f'{int(freq)}',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout for the current site_id and pump_id\n",
    "    fig3.update_layout(\n",
    "        title=f'Health Score over Time (Site: {site_id}, Pump: {pump_id})',\n",
    "        xaxis_title='Timestamp',\n",
    "        yaxis_title='Health Score',\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        width=1600,\n",
    "    )\n",
    "\n",
    "    # Display the figure for this site_id, pump_id pair\n",
    "    fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in your dataframe\n",
    "timestamp_column = 'timestamp'\n",
    "# frequency_int_column = 'frequency'\n",
    "health_score_column = 'health_score'\n",
    "normalized_health_score_column = 'normalized_health_score'\n",
    "\n",
    "calibration_stages = site_info['calibration_stages']\n",
    "\n",
    "plot_slice = 1\n",
    "\n",
    "\n",
    "def interpolate_missing_freqs(freq_dict):\n",
    "    \"\"\"\n",
    "    Interpolates means and stds for missing frequencies.\n",
    "    freq_dict should be {freq: {'mean':..., 'std':...}}\n",
    "    \"\"\"\n",
    "    freqs = sorted(freq_dict.keys())\n",
    "    if len(freqs) <= 1:\n",
    "        # If we have only one or zero frequencies, no interpolation is possible\n",
    "        return freq_dict\n",
    "\n",
    "    full_freq_range = range(freqs[0], freqs[-1] + 1)\n",
    "\n",
    "    for f in full_freq_range:\n",
    "        if f not in freq_dict:\n",
    "            # Find lower and higher available frequencies\n",
    "            lower_freqs = [ff for ff in freqs if ff < f]\n",
    "            higher_freqs = [ff for ff in freqs if ff > f]\n",
    "            if not lower_freqs or not higher_freqs:\n",
    "                # Cannot interpolate if no lower or higher freq exists\n",
    "                continue\n",
    "            lower_f = max(lower_freqs)\n",
    "            higher_f = min(higher_freqs)\n",
    "            # Linear interpolation of mean and std\n",
    "            w = (f - lower_f) / (higher_f - lower_f)\n",
    "            mean_val = freq_dict[lower_f]['mean'] + w * (freq_dict[higher_f]['mean'] - freq_dict[lower_f]['mean'])\n",
    "            std_val = freq_dict[lower_f]['std'] + w * (freq_dict[higher_f]['std'] - freq_dict[lower_f]['std'])\n",
    "            freq_dict[f] = {\n",
    "                'mean': mean_val,\n",
    "                'std': std_val\n",
    "            }\n",
    "\n",
    "    return freq_dict\n",
    "\n",
    "# Iterate over each site_id and pump_id pair\n",
    "for (site_id, pump_id), group_df in df.groupby(level=['site_id','pump_id']):\n",
    "    # Reset index to timestamp\n",
    "    df_plot = group_df.reset_index().set_index(timestamp_column)\n",
    "    df_plot.index = pd.to_datetime(df_plot.index)\n",
    "\n",
    "    # Compute mean and std from calibration intervals\n",
    "    freq_dict = {}\n",
    "    for stage in calibration_stages:\n",
    "        freq = stage['frequency']\n",
    "        start = pd.to_datetime(stage['start_time'])\n",
    "        end = pd.to_datetime(stage['end_time'])\n",
    "        \n",
    "        # Filter the data for this frequency and time window\n",
    "        freq_cal_data = df_plot[\n",
    "            (df_plot.index >= start) &\n",
    "            (df_plot.index <= end)\n",
    "        ]\n",
    "        \n",
    "        freq_mean = freq_cal_data[health_score_column].mean() if not freq_cal_data.empty else np.nan\n",
    "        freq_std = freq_cal_data[health_score_column].std() if not freq_cal_data.empty else np.nan\n",
    "        \n",
    "        freq_dict[freq] = {\n",
    "            'mean': freq_mean,\n",
    "            'std': freq_std\n",
    "        }\n",
    "\n",
    "    # Interpolate missing frequencies if needed\n",
    "    freq_dict = interpolate_missing_freqs(freq_dict)\n",
    "\n",
    "    # Normalize the health score\n",
    "    def normalize_health_score(row):\n",
    "        f = row[frequency_int_column]\n",
    "        if f not in freq_dict:\n",
    "            return np.nan\n",
    "        mean_val = freq_dict[f]['mean']\n",
    "        std_val = freq_dict[f]['std']\n",
    "        if pd.isna(mean_val) or pd.isna(std_val) or std_val == 0:\n",
    "            return np.nan\n",
    "        return (row[health_score_column] - mean_val) / std_val\n",
    "\n",
    "    df_plot[normalized_health_score_column] = df_plot.apply(normalize_health_score, axis=1)\n",
    "\n",
    "    # At this point, df_plot has 'normalized_health_score' and is indexed by timestamp.\n",
    "    # We now reset the index so we can merge back into df.\n",
    "    df_plot_reset = df_plot.reset_index()\n",
    "    # Now df_plot_reset has columns ['timestamp', 'site_id', 'pump_id', 'health_score', 'normalized_health_score', ...]\n",
    "    # Set the multi-index to match original df index\n",
    "    df_plot_reset.set_index(['site_id', 'pump_id', timestamp_column], inplace=True)\n",
    "\n",
    "    # Merge normalized values back into df\n",
    "    df.loc[df_plot_reset.index, normalized_health_score_column] = df_plot_reset[normalized_health_score_column]\n",
    "\n",
    "    # Now that df is updated, we can proceed to plotting from df or df_plot\n",
    "    current_frequencies = sorted(df_plot[frequency_int_column].unique(), reverse=True)\n",
    "\n",
    "    fig3 = go.Figure()\n",
    "\n",
    "    # start_time = '2024-12-13'\n",
    "    # end_time = '2024-12-15'\n",
    "    for freq in current_frequencies:\n",
    "        freq_data = df_plot[df_plot[frequency_int_column] == freq]#.loc[start_time:end_time]\n",
    "\n",
    "        # Use the previously defined color_map\n",
    "        if freq not in color_map:\n",
    "            continue\n",
    "\n",
    "        fig3.add_trace(\n",
    "            go.Scatter(\n",
    "                x=freq_data[::plot_slice].index,\n",
    "                y=freq_data[normalized_health_score_column][::plot_slice],\n",
    "                mode='markers',\n",
    "                name=f'{int(freq)}Hz',\n",
    "                marker=dict(color=color_map[freq]),\n",
    "                legendgroup=f'{int(freq)}',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig3.update_layout(\n",
    "        title=f'Normalized Health Score over Time (Site: {site_id}, Pump: {pump_id})',\n",
    "        xaxis_title='Timestamp',\n",
    "        yaxis_title='Normalized Health Score',\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        width=1600,\n",
    "    )\n",
    "    fig3.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "    fig3.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
    "\n",
    "    fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Mean +- 3STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_health_score_column = 'health_score'\n",
    "normalized_health_score_column = 'normalized_health_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.set_index(['site_id','pump_id','timestamp'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters near the top\n",
    "alert_threshold = 2\n",
    "\n",
    "\n",
    "# Define rolling window parameters\n",
    "long_window = '14D'\n",
    "short_window = '6H'\n",
    "\n",
    "# Iterate over each site_id and pump_id pair\n",
    "results = {}  # Store computed data, including events, for each site/pump\n",
    "\n",
    "for (site_id, pump_id), group_df in df.groupby(level=['site_id', 'pump_id']):\n",
    "    # Reset index to timestamp\n",
    "    df_plot = group_df.reset_index().set_index(timestamp_column).sort_index()\n",
    "\n",
    "    # Compute long rolling statistics (for thresholds)\n",
    "    rolling_long_mean = df_plot[normalized_health_score_column].rolling(long_window, closed='left').mean()\n",
    "    rolling_long_std = df_plot[normalized_health_score_column].rolling(long_window, closed='left').std()\n",
    "    upper_bound = rolling_long_mean + alert_threshold * rolling_long_std\n",
    "    lower_bound = rolling_long_mean - alert_threshold * rolling_long_std\n",
    "\n",
    "    # Compute short rolling statistic (used for event detection)\n",
    "    rolling_short_mean = df_plot[normalized_health_score_column].rolling(short_window, closed='left').mean()\n",
    "\n",
    "    # Detect events\n",
    "    # Event starts when rolling_short_mean goes outside the threshold (above upper or below lower)\n",
    "    # Event ends when rolling_short_mean returns inside the threshold.\n",
    "    out_of_range = (rolling_short_mean > upper_bound) | (rolling_short_mean < lower_bound)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index(['timestamp'],inplace=True)\n",
    "    df['anomaly'] = out_of_range\n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index(['site_id','pump_id','timestamp'],inplace=True)\n",
    "    event_intervals = []\n",
    "    in_event = False\n",
    "    event_start = None\n",
    "    \n",
    "    for t, outside in out_of_range.items():\n",
    "        if outside and not in_event:\n",
    "            # Start event\n",
    "            in_event = True\n",
    "            event_start = t\n",
    "        elif not outside and in_event:\n",
    "            # End event\n",
    "            in_event = False\n",
    "            event_end = t\n",
    "            event_intervals.append((event_start, event_end))\n",
    "            \n",
    "    # If still in_event by the end, close it at the last timestamp\n",
    "    if in_event:\n",
    "        event_end = df_plot.index[-1]\n",
    "        event_intervals.append((event_start, event_end))\n",
    "\n",
    "    # Store computed data and events\n",
    "    results[(site_id, pump_id)] = {\n",
    "        'df_plot': df_plot,\n",
    "        'rolling_long_mean': rolling_long_mean,\n",
    "        'rolling_long_std': rolling_long_std,\n",
    "        'upper_bound': upper_bound,\n",
    "        'lower_bound': lower_bound,\n",
    "        'rolling_short_mean': rolling_short_mean,\n",
    "        'event_intervals': event_intervals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Code Cell (Plotting)\n",
    "plot_slice = 10\n",
    "\n",
    "for (site_id, pump_id), res in results.items():\n",
    "    df_plot = res['df_plot']\n",
    "    rolling_long_mean = res['rolling_long_mean']\n",
    "    rolling_long_std = res['rolling_long_std']\n",
    "    upper_bound = res['upper_bound']\n",
    "    lower_bound = res['lower_bound']\n",
    "    rolling_short_mean = res['rolling_short_mean']\n",
    "    event_intervals = res['event_intervals']\n",
    "\n",
    "    fig3 = go.Figure()\n",
    "\n",
    "    # Get unique frequencies for the current group\n",
    "    current_frequencies = sorted(df_plot[frequency_int_column].unique(), reverse=True)\n",
    "\n",
    "    for freq in current_frequencies:\n",
    "        freq_data = df_plot[df_plot[frequency_int_column] == freq]\n",
    "\n",
    "        # Only plot if freq is in color_map\n",
    "        if freq not in color_map:\n",
    "            continue\n",
    "\n",
    "        # Add scatter plot for raw health scores\n",
    "        fig3.add_trace(\n",
    "            go.Scatter(\n",
    "                x=freq_data[::plot_slice].index,  # Use index as x-axis (timestamp)\n",
    "                y=freq_data[normalized_health_score_column][::plot_slice],\n",
    "                mode='markers',\n",
    "                name=f'{int(freq)}Hz',\n",
    "                marker=dict(color=color_map[freq]),\n",
    "                legendgroup=f'{int(freq)}',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Plot the long rolling mean and thresholds once (they are frequency-agnostic)\n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rolling_long_mean.index,\n",
    "            y=rolling_long_mean,\n",
    "            mode='lines',\n",
    "            name='Long Mean',\n",
    "            line=dict(color='#FFFFFF', width=5),\n",
    "            legendgroup='stats',\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rolling_long_mean.index,\n",
    "            y=rolling_long_mean,\n",
    "            mode='lines',\n",
    "            name='Long Mean',\n",
    "            line=dict(color='#FF0000', width=1, dash='dash'),\n",
    "            legendgroup='stats',\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=upper_bound.index,\n",
    "            y=upper_bound,\n",
    "            mode='lines',\n",
    "            name='± Threshold',\n",
    "            line=dict(color='#FF0000', width=1, dash='dash'),\n",
    "            legendgroup='stats',\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=lower_bound.index,\n",
    "            y=lower_bound,\n",
    "            mode='lines',\n",
    "            name='± Threshold',\n",
    "            line=dict(color='#FF0000', width=1, dash='dash'),\n",
    "            legendgroup='stats',\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add short rolling mean\n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rolling_short_mean.index,\n",
    "            y=rolling_short_mean,\n",
    "            mode='lines',\n",
    "            name='Short Mean',\n",
    "            line=dict(color='black', width=2),\n",
    "            legendgroup='stats',\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add event annotations\n",
    "    # We'll represent each event interval as a filled area. To make them toggle together,\n",
    "    # use a single legendgroup and only showlegend for the first event.\n",
    "    show_legend_for_events = True\n",
    "    y_min = min(df_plot[normalized_health_score_column].min(), lower_bound.min(), upper_bound.min())\n",
    "    y_max = max(df_plot[normalized_health_score_column].max(), upper_bound.max(), lower_bound.max())\n",
    "\n",
    "    for (start, end) in event_intervals:\n",
    "        fig3.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[start, start, end, end],\n",
    "                y=[y_min, y_max, y_max, y_min],\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(255,0,0,0.1)',  # red with 10% alpha\n",
    "                line=dict(width=0),\n",
    "                name='Events' if show_legend_for_events else None,\n",
    "                legendgroup='Events',\n",
    "                showlegend=show_legend_for_events\n",
    "            )\n",
    "        )\n",
    "        show_legend_for_events = False\n",
    "\n",
    "    # Update layout for the current site_id and pump_id\n",
    "    fig3.update_layout(\n",
    "        title=f'Health Score over Time (Site: {site_id}, Pump: {pump_id})',\n",
    "        xaxis_title='Timestamp',\n",
    "        yaxis_title='Health Score',\n",
    "        template='plotly_white',\n",
    "        height=500,\n",
    "        width=1600,\n",
    "    )\n",
    "\n",
    "    fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to collect event details\n",
    "event_details = []\n",
    "\n",
    "for (site_id, pump_id), res in results.items():\n",
    "    df_plot = res['df_plot']\n",
    "    rolling_short_mean = res['rolling_short_mean']\n",
    "    upper_bound = res['upper_bound']\n",
    "    lower_bound = res['lower_bound']\n",
    "    event_intervals = res['event_intervals']\n",
    "    \n",
    "    for (start, end) in event_intervals:\n",
    "        # Determine the event type based on the rolling_short_mean at the start of the event\n",
    "        try:\n",
    "            value_at_start = rolling_short_mean.loc[start]\n",
    "            upper = upper_bound.loc[start]\n",
    "            lower = lower_bound.loc[start]\n",
    "        except KeyError:\n",
    "            # Handle cases where the start time might not align perfectly\n",
    "            value_at_start = rolling_short_mean.iloc[rolling_short_mean.index.get_loc(start, method='nearest')]\n",
    "            upper = upper_bound.iloc[rolling_short_mean.index.get_loc(start, method='nearest')]\n",
    "            lower = lower_bound.iloc[rolling_short_mean.index.get_loc(start, method='nearest')]\n",
    "        \n",
    "        if value_at_start > upper:\n",
    "            event_type = 'Above Threshold'\n",
    "        elif value_at_start < lower:\n",
    "            event_type = 'Below Threshold'\n",
    "        else:\n",
    "            event_type = 'Unknown'  # Fallback in case it's neither\n",
    "        \n",
    "        # Calculate duration\n",
    "        duration = end - start\n",
    "        \n",
    "        # Append the event details to the list\n",
    "        event_details.append({\n",
    "            'Site ID': site_id,\n",
    "            'Pump ID': pump_id,\n",
    "            'Start Time': start,\n",
    "            'End Time': end,\n",
    "            'Duration': duration,\n",
    "            'Event Type': event_type\n",
    "        })\n",
    "\n",
    "# Convert the list of events to a DataFrame\n",
    "events_df = pd.DataFrame(event_details)\n",
    "\n",
    "# Optional: Format the duration as total hours or another preferred unit\n",
    "events_df['Duration (Hours)'] = events_df['Duration'].dt.total_seconds() / 3600\n",
    "\n",
    "# Rearrange columns for better readability\n",
    "events_df = events_df[[\n",
    "    'Site ID',\n",
    "    'Pump ID',\n",
    "    'Start Time',\n",
    "    'End Time',\n",
    "    'Duration (Hours)',\n",
    "    'Event Type'\n",
    "]]\n",
    "\n",
    "# Display the events DataFrame\n",
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output base path\n",
    "output_base_path = '/Volumes/RyanMercerTB3/dev_RyanMercer/datasets/Bison/DailyReports/PercentFromBEP/'  # Replace with your desired output path\n",
    "\n",
    "# Ensure the output base path exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Get the current date\n",
    "date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "target_column = 'health_score'\n",
    "annotation_column = 'anomaly'\n",
    "\n",
    "\n",
    "\n",
    "# **Output Results to CSV Files**\n",
    "for site_info in sites_info:\n",
    "    site_id = site_info['site_id']\n",
    "    site_name = site_info['site_name']\n",
    "\n",
    "    # Get the list of pump_ids for the current site_id\n",
    "    pump_ids = df.xs(site_id, level='site_id').index.get_level_values('pump_id').unique()\n",
    "\n",
    "    # Sanitize the site_name for the file name (if necessary)\n",
    "    site_name_clean = str(site_name).replace('/', '_').replace('(', '').replace(')', '').replace(' ', '_')\n",
    "\n",
    "    for pump_id in pump_ids:\n",
    "        # Select data for the current site and pump\n",
    "        df_pump = df.xs((site_id, pump_id), level=('site_id', 'pump_id')).copy()\n",
    "\n",
    "        # Check if necessary columns exist for this pump\n",
    "        required_columns = [target_column, annotation_column]\n",
    "        missing_columns = [col for col in required_columns if col not in df_pump.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Columns {missing_columns} not found for site_id {site_id}, pump_id {pump_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare the output DataFrame\n",
    "        df_output = df_pump[[target_column, annotation_column]].copy()\n",
    "        # df_output.rename(columns={annotation_column: 'downtime'}, inplace=True)\n",
    "        # df_output['downtime'] = df_output['downtime'].astype(int)\n",
    "\n",
    "        # Reset index to include 'Timestamp' as a column\n",
    "        df_output.reset_index(inplace=True)\n",
    "\n",
    "        # **Ensure that 'Timestamp' is a datetime with timezone awareness**\n",
    "        if df_output['timestamp'].dtype == 'datetime64[ns]':\n",
    "            df_output['timestamp'] = df_output['timestamp'].dt.tz_localize('UTC')\n",
    "\n",
    "        # **Check if there is an active alert (an alert in the last 24 hours of data)**\n",
    "        # Use the maximum timestamp in the data\n",
    "        max_timestamp = df_output['timestamp'].max()\n",
    "        last_24_hours = max_timestamp - pd.Timedelta(hours=24)\n",
    "\n",
    "        # Now, check if there are any anomalies in the last 24 hours of data\n",
    "        recent_anomalies = df_output[(df_output['timestamp'] >= last_24_hours) & (df_output['anomaly'] == 1)]\n",
    "        if not recent_anomalies.empty:\n",
    "            downtime_suffix = '_anomaly'\n",
    "            downtime_path_suffix = 'anomaly'\n",
    "        else:\n",
    "            downtime_suffix = ''\n",
    "            downtime_path_suffix = 'nonanomaly'\n",
    "\n",
    "        # Construct the file name\n",
    "        filename = f\"site_{site_name_clean}_{site_id}_pump_{int(pump_id)}_BEPPercent_{date}{downtime_suffix}.csv\"\n",
    "\n",
    "        # Full path to save the CSV file\n",
    "        site_output_path = os.path.join(output_base_path, date, downtime_path_suffix)\n",
    "        os.makedirs(site_output_path, exist_ok=True)\n",
    "        filepath = os.path.join(site_output_path, filename)\n",
    "\n",
    "        # Output column order\n",
    "        desired_column_order = ['timestamp', target_column,'anomaly']\n",
    "        # Save to CSV\n",
    "        df_output.to_csv(filepath, index=False, columns=desired_column_order)\n",
    "\n",
    "        print(f\"Saved data for site {site_id}, pump {pump_id} to {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
